-- add blink-cmp-avante to blink.cmp
-- setup TAVILY_API_KEY env variable
return {
  {
    "yetone/avante.nvim",
    event = "VeryLazy",
    version = false, -- Never set this value to "*"! Never!
    build = "make",
    dependencies = {
      "nvim-treesitter/nvim-treesitter",
      "stevearc/dressing.nvim",
      "nvim-lua/plenary.nvim",
      "MunifTanjim/nui.nvim",
      --- The below dependencies are optional,
      -- "echasnovski/mini.pick",           -- for file_selector provider mini.pick
      -- "nvim-telescope/telescope.nvim",   -- for file_selector provider telescope
      -- "hrsh7th/nvim-cmp",                -- autocompletion for avante commands and mentions
      -- "ibhagwan/fzf-lua",                -- for file_selector provider fzf
      "nvim-tree/nvim-web-devicons", -- or echasnovski/mini.icons
      "zbirenbaum/copilot.lua",      -- for providers='copilot'
      -- {
      --   -- support for image pasting
      --   "HakonHarnes/img-clip.nvim",
      --   event = "VeryLazy",
      --   opts = {
      --     -- recommended settings
      --     default = {
      --       embed_image_as_base64 = false,
      --       prompt_for_file_name = false,
      --       drag_and_drop = {
      --         insert_mode = true,
      --       },
      --       -- required for Windows users
      --       use_absolute_path = true,
      --     },
      --   },
      -- },
    },
    opts = {
      -- add any opts here
      -- for example
      provider = "copilot",
      -- -- auto_suggestions_provider = "openai",
      -- openai = {
      --   endpoint = "https://api.openai.com/v1",
      --   model = "gpt-4o",
      --   timeout = 30000,               -- Timeout in milliseconds, increase this for reasoning models
      --   temperature = 0,
      --   max_completion_tokens = 16384, -- Increase this to include reasoning tokens (for reasoning models)
      --   reasoning_effort = "medium",   -- low|medium|high, only used for reasoning models
      -- },
      copilot = {
        endpoint = "https://api.githubcopilot.com",
        model = "gemini-2.5-pro-preview-03-25",
        proxy = nil,            -- [protocol://]host[:port] Use this proxy
        allow_insecure = false, -- Allow insecure server connections
        timeout = 30000,        -- Timeout in milliseconds
        temperature = 0,
        -- max_tokens = 50000
      },
      web_search_engine = {
        provider = "tavily", -- tavily, serpapi, searchapi, google, kagi, brave, or searxng
        proxy = nil,         -- proxy support, e.g., http://127.0.0.1:7890
      },
      cursor_applying_provider = "openai",
      behaviour = {
        enable_cursor_planning_mode = true,
      }
    },

    hints = {
      enabled = false
    },

    rag_service = {
      enabled = true,                             -- Enables the RAG service
      host_mount = os.getenv("HOME"),             -- Host mount path for the rag service
      provider = "copilot",                       -- The provider to use for RAG service (e.g. openai or ollama)
      llm_model = "gemini-2.5-pro-preview-03-25", -- The LLM model to use for RAG service
      -- embed_model = "",                       -- The embedding model to use for RAG service
      -- endpoint = "https://api.openai.com/v1",     -- The API endpoint for RAG service
    },
  }
}
